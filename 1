import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch
import torchvision
import torchvision.transforms as transforms
from torchvision.transforms import ToPILImage
show=ToPILImage()

#1.准备训练数据与测试数据
transform = transforms.Compose(#整合多个操作
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])#R,G,B的均值和方差
# ToTensor()将图片转化为Tensor，Normalize()将图片像素值归一化到[-1,1]之间

batch_size=4
'''
root:指定数据集所在路径
train:True表示使用训练集，False表示使用测试集
download:True表示下载数据集，False表示不下载
transform:应用先前的transform进行数据预处理操作
dataloader:创建数据加载器
batch_size:每次加载的数据量
shuffle表示是否打乱数据顺序，增加模型的泛化能力
num_workers表示加载数据时的线程数，可以加速数据加载过程
'''
# 加载CIFAR10数据集
trainset=torchvision.datasets.CIFAR10(root='./data',
                                      train=True,
                                      download=True,
                                      transform=transform)
trainloader=torch.utils.data.DataLoader(trainset,
                                        batch_size=batch_size,
                                        shuffle=True,
                                        num_workers=0)
testset=torchvision.datasets.CIFAR10(root='./data',
                                     train=False,
                                     download=True,
                                     transform=transform)
testloader=torch.utils.data.DataLoader(testset,
                                       batch_size=batch_size,
                                       shuffle=False,
                                       num_workers=0)


classes=('plane', 'car', 'bird', 'cat','deer','dog','frog','horse','ship','truck')
print(len(trainset))
print(len(testset))
print(trainset[0][0].size())#第一张图片的大小
print(trainset[0][1])#第一张图片的标签
print(classes[trainset[0][1]])#

# 1 - 4行与上一块代码意义类似
(data, label) = trainset[12] # 选群训练集的一个样本展示内容，也可以改成其他数字看看
print(data.size())
print(label) # label是整数
print(classes[label])
show((data + 1) / 2).resize((100, 100)) #将图片像素值恢复到[0,1]之间（标准正态分布）并resize成100x100

#2.定义用于分类的网络结构
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self):
        # nn.Module子类的函数必须在构造函数中执行父类的构造函数
        super(Net, self).__init__()

        # 卷积层 '3'表示输入图片为单通道, '6'表示输出通道数，'5'表示卷积核为5*5
        self.conv1 = nn.Conv2d(3, 6, 5)
        # 卷积层
        self.conv2 = nn.Conv2d(6, 16, 5)
        # 仿射层/全连接层，y = Wx + b
        self.fc1   = nn.Linear(16*5*5, 120)
        self.fc2   = nn.Linear(120, 84)
        self.fc3   = nn.Linear(84, 10)

    def forward(self, x):
        # 卷积 -> 激活 -> 池化 (relu激活函数不改变输入的形状)
        # [batch size, 3, 32, 32] -- conv1 --> [batch size, 6, 28, 28] -- max pool --> [batch size, 6, 14, 14]
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        # [batch size, 6, 14, 14] -- conv2 --> [batch size, 16, 10, 10] --> max pool --> [batch size, 16, 5, 5]
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        # 把 16 * 5 * 5 的特征图展平，变为 [batch size, 16 * 5 * 5]，以送入全连接层
        x = x.view(x.size()[0], -1)
        # [batch size, 16 * 5 * 5] -- fc1 --> [batch size, 120]
        x = F.relu(self.fc1(x))
        # [batch size, 120] -- fc2 --> [batch size, 84]
        x = F.relu(self.fc2(x))
        # [batch size, 84] -- fc3 --> [batch size, 10]
        x = self.fc3(x)
        return x

net = Net()
print(net)

#3.模型训练与测试过程
from torch import optim
criterion = nn.CrossEntropyLoss() # 交叉熵损失函数
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) # 使用SGD（随机梯度下降）优化（也可使用Adam、Adagrad等）
num_epochs = 5 # 训练5轮

loss_values=[]

def train(trainloader, net, num_epochs, criterion, optimizer, save_path):
    for epoch in range(num_epochs):
        running_loss = 0.0
        for i, data in enumerate(trainloader, 0):

            # 1. 取出数据
            inputs, labels = data

            # 梯度清零
            optimizer.zero_grad()

            # 2. 前向计算和反向传播
            outputs = net(inputs)  # 送入网络（正向传播）
            loss = criterion(outputs, labels)  # 计算损失函数

            # 3. 反向传播，更新参数
            loss.backward()  # 反向传播
            optimizer.step()  # 更新模型的参数，根据之前计算得到的梯度进行参数优化
            # 4. 记录损失值
            loss_values.append(loss.item())

            # 下面的这段代码对于训练无实际作用，仅用于观察训练状态
            running_loss += loss.item()
            if i % 1000 == 999:  # 每1000个batch打印一下训练状态
                print('epoch %d: batch %5d loss: %.3f' \
                      % (epoch + 1, i + 1, running_loss / 1000))
                running_loss = 0.0

        torch.save(net.state_dict(), f"{save_path}/epoch_{epoch + 1}_model.pth")
    print('Finished Training')



# 使用定义的网络进行训练
save_path = r'E:\\dataset'
train(trainloader, net, num_epochs, criterion, optimizer, save_path)

def predict(testloader, net):
    correct = 0  # 预测正确的图片数
    total = 0  # 总共的图片数

    with torch.no_grad():  # 正向传播时不计算梯度
        for data in testloader:
            # 1. 取出数据
            images, labels = data
            # 2. 正向传播，得到输出结果
            outputs = net(images)
            # 3. 从输出中得到模型预测
            _, predicted = torch.max(outputs, 1)  # 1 表示沿着列的方向，返回两个值：最大值和对应的索引
            # 4. 计算性能指标
            total += labels.size(0)  # size(0) 返回张量的第一个维度的大小，即当前批次中图片的数量
            correct += (predicted == labels).sum()

    print('测试集中的准确率为: %d %%' % (100 * correct / total))
predict(testloader, net)


#好的，现在所有的函数已经准备完毕，开始task吧！
import matplotlib.pyplot as plt
import numpy as np
#绘图
def draw(values):
    plt.plot(values)
    plt.show()

draw(loss_values)

#task2.1
# TODO: 在Dropout_Net中加入dropout层
class Dropout_Net(nn.Module):
    def __init__(self):
        # nn.Module子类的函数必须在构造函数中执行父类的构造函数
        super(Dropout_Net, self).__init__()

        # 卷积层 '1'表示输入图片为单通道, '6'表示输出通道数，'5'表示卷积核为5*5
        self.conv1 = nn.Conv2d(3, 6, 5)
        # 卷积层
        self.conv2 = nn.Conv2d(6, 16, 5)
        # 仿射层/全连接层，y = Wx + b
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.dropout = nn.Dropout(p=0.5)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # 卷积 -> 激活 -> 池化
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        # reshape，‘-1’表示自适应
        x = x.view(x.size()[0], -1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


dropout_net = Dropout_Net()


#task2.2
# TODO: 在这里重新定义optimizer，加入L2正则化项
optimizer = optim.SGD(dropout_net.parameters(),lr=0.001, momentum=0.9, weight_decay=5e-4)
# 使用新定义的网络和优化器进行训练与测试
train(trainloader, dropout_net, num_epochs, criterion, optimizer)
predict(testloader, dropout_net)

#task3
#报错处空格待修改
num_epochs =3
optimizer = optim.SGD(net.parameters(), lr=0.003, momentum=0.9, weight_decay=5e-4)
